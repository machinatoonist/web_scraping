---
title: "TidyTemplate"
date: 2021-06-30
output: html_output
---

# TidyTuesday

Join the R4DS Online Learning Community in the weekly #TidyTuesday event!
Every week we post a raw dataset, a chart or article related to that dataset, and ask you to explore the data.
While the dataset will be “tamed”, it will not always be tidy! As such you might need to apply various R for Data Science techniques to wrangle the data into a true tidy format.
The goal of TidyTuesday is to apply your R skills, get feedback, explore other’s work, and connect with the greater #RStats community!
As such we encourage everyone of all skills to participate!

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidytuesdayR)
library(ggpomological)
library(tidytext)
library(rvest)
library(widyr)
library(ggraph)
library(igraph)
theme_set(theme_pomological_fancy())


tidytuesdayR::tt_available()

```

# Load the weekly Data

Dowload the weekly data and make available in the `tt` object.

```{r Load}

tt <- tt_load("2018-12-04")

```


# Readme

Take a look at the readme for the weekly data to get insight on the dataset.
This includes a data dictionary, source, and a link to an article on the data.

```{r Readme, eval = interactive()}

tt

```


# Glimpse Data

Take an initial look at the format of the data available.

```{r Glimpse}

tt %>% 
  map(glimpse)

```

# Wrangle

Explore the data and process it into a nice format for plotting! Access each dataset by name by using a dollarsign after the `tt` object and then the name of the data set.

```{r Wrangle}
medium_datasci <- tt[[1]]

medium_prep_tbl <- medium_datasci %>%
  select(-x1) %>%
  mutate(post_id = row_number())

write_csv(medium_datasci, "data/medium_datasci.csv")
```

```{r}
medium_prep_tbl %>%
  count(publication, sort = TRUE)

medium_prep_tbl %>%
  count(author, sort = TRUE)

medium_prep_tbl %>%
  summarize_at(vars(starts_with("tag")), sum)

medium_prep_tbl %>%
  count(author_url, sort = TRUE)


```

# Which tags get the most claps?
```{r}
medium_long_tbl <- medium_prep_tbl %>%
  pivot_longer(cols = starts_with("tag"), 
               names_to = "tag",
               values_to = "value") %>%
  mutate(tag = str_remove(tag, "tag_")) %>%
  filter(value == 1)

medium_long_tbl %>%
  count(tag, sort = TRUE)

medium_long_tbl %>%
  group_by(tag) %>%
  summarise(median_claps = median(claps)) %>%
  arrange(desc(median_claps))

medium_prep_tbl %>%
  ggplot(aes(claps)) +
  geom_histogram() +
  scale_x_log10(labels = scales::comma_format())

medium_prep_tbl %>%
  ggplot(aes(reading_time)) +
  geom_histogram() +
  scale_x_log10(labels = scales::comma_format())

medium_prep_tbl %>%
  # Mutate reading time to be no greater than 10
  mutate(reading_time = pmin(10, reading_time)) %>%
  ggplot(aes(reading_time)) +
  geom_histogram(binwidth = 0.5) +
  scale_x_continuous(breaks = seq(2, 10, 2),
                     labels = c(seq(2, 8, 2), "10+")) +
  labs(x = "Medium reading time")
  
medium_long_tbl %>%
  group_by(tag) %>%
  summarize(reading_time = mean(reading_time)) %>%
  arrange(desc(reading_time))

  
```

## Text Mining

```{r}

# Each word gets a row with a post_id next to it
medium_words <- medium_prep_tbl %>%
  filter(!is.na(title)) %>%
  select(post_id, title, subtitle, year, reading_time, claps) %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words, by = "word") %>%
  filter(!(word %in% c("de", "en", "la", "para")),
         str_detect(word, "[a-z]"))  # Must have at least one letter


medium_words %>%
  count(word, sort = TRUE) %>%
  mutate(word = fct_reorder(word, n)) %>%
  head(20) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  labs(title = "Common words in Medium Post Titles")
  

```

```{r}
medium_words_filtered <- medium_words %>%
  add_count(word) %>%
  filter(n >= 250) 

tag_claps <- medium_words_filtered %>%
  group_by(word) %>%
  summarize(median_claps = median(claps),
            geometric_mean_claps = exp(mean(log(claps + 1))) - 1,
            occurences = n()) %>%
  arrange(desc(median_claps))


top_word_cors <- medium_words_filtered %>%
  select(post_id, word) %>%
  pairwise_cor(word, post_id, sort = TRUE) %>%
  head(150)

vertices <- tag_claps %>%
  filter(word %in% top_word_cors$item1 |
           word %in% top_word_cors$item2)

set.seed(2018)
top_word_cors %>%
  graph_from_data_frame(vertices = vertices) %>%
  ggraph(layout = 'fr') +
  geom_edge_link() +
  geom_node_point(aes(size = occurences * 1.1,
                      color = geometric_mean_claps)) +
  geom_node_text(aes(label = name), repel = TRUE) +
  scale_color_gradient2(low = "blue",
                        high = "red",
                        midpoint = 10) +
  theme_void() +
  labs(title = "Popular Words in Data Science Related Medium Article Titles in 2017-18",
       subtitle = "Color shows the average number of claps on articles with these words in the title",
       size = "# of occurrences",
       color = "Claps (geometric mean)", 
       caption = "Dataset from: https://www.kaggle.com/harrisonjansma/medium-stories via #tidytuesday")

medium_words_filtered %>% distinct(word)

```

## Predicting the number of claps based on title and tag

```{r}
# Create a sparse matrix of words
# One row per post, one column per word
post_word_matrix <- medium_words_filtered %>%
  distinct(post_id, word, claps) %>%
  cast_sparse(row = post_id, column = word)

library(tidymodels)
library(glmnet)

parsnip::linear_reg()

# Number of claps for every post
claps <- medium_prep_tbl$claps[match(rownames(post_word_matrix), medium_prep_tbl$post_id)]

lasso_model <- cv.glmnet(post_word_matrix, log(claps + 1))

# Regularisation

```


```{r}
plot(lasso_model)

library(broom)
# Influence of the word on the number of claps in a machine learning model


tidy(lasso_model)

tidy(lasso_model$glmnet.fit) %>% 
  filter(term %in% c("reinforcement", "deep", "learning", "gdpr", "business")) %>%
  ggplot(aes(lambda, estimate, color = term)) +
  geom_line() +
  scale_x_log10()

lasso_model$lambda.min

tidy(lasso_model$glmnet.fit) %>% 
  filter(lambda == lasso_model$lambda.min) %>%
  arrange(desc(estimate)) %>% View()
  
# Build a model to predict how many claps you will get based on your title

```



```{r}
library(tidygraph)

graph <- as_tbl_graph(
  data.frame(
    from = sample(5, 20, TRUE),
    to = sample(5, 20, TRUE),
    weight = runif(20)
  )
)
graph

ggraph(graph, layout = 'fr', weights = weight) + 
  geom_edge_link() + 
  geom_node_point()

```


```{r}
library(rvest)
library(dplyr)
content_1 <- read_html("https://towardsdatascience.com/")

b_text <- content_1 %>% html_nodes("body") %>% html_text()
```




# Visualize

Using your processed dataset, create your unique visualization.

```{r Visualize}


  
```

# Save Image

Save your image for sharing. Be sure to use the `#TidyTuesday` hashtag in your post on twitter! 

```{r}

# This will save your most recent plot
ggsave(
  filename = "My TidyTuesday Plot.png",
  device = "png")

```
