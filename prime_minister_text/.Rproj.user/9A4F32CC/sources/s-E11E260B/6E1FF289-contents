---
title: "TidyTemplate"
date: 2021-07-10
output: html_output
---

# MLMondays

See also Sydney Airbnb dataset
https://www.kaggle.com/tylerx/sydney-airbnb-open-data?select=listings_summary_dec18.csv
- contains lat/lon, price, 
- Create a ggmap plot of Sydney Airbnb prices by lat/lon

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
library(textrecipes)
library(scales)
library(skimr)
library(esquisse)
library(GGally)
library(stacks)
library(ggmap)
library(patchwork)
theme_set(theme_light())

doParallel::registerDoParallel(cores = 4)

```

# Import Data
```{r}
dataset <- read_csv("train.csv", guess_max = 1000) %>%
    mutate(price = log1p(price))
holdout <- read_csv("test.csv")

dataset %>% glimpse()

```

```{r}
dataset %>%
    skim()
```

# Numerical relationships
```{r}
dataset %>%
  select(neighbourhood_group, price, latitude, longitude, minimum_nights, number_of_reviews, calculated_host_listings_count, availability_365) %>%
  ggpairs(columns = 2:5, aes(color = neighbourhood_group, alpha = 0.5))
```

# Split into train and test set
```{r}
set.seed(42)
splits <- initial_split(dataset, 0.75)
train <-  training(splits)
test <- testing(splits)
```

```{r}
train
test
holdout
```

# Prediction function for holdout
```{r}
library(broom)
predict_holdout <- function(wf) {
    wf %>%
        augment(holdout) %>%
        mutate(.pred = exp(.pred) - 1) %>%
        select(id, price = .pred)
}

augment.model_stack <- function(x, data, ...) {
  bind_cols(data, predict(x, data, ...))
}
```

```{r}
train %>%
    count(name, sort = TRUE)

train %>% View()

# No price data in holdout set
holdout

```

```{r}
# Create function to summarise by groups
summarise_prices <- function(tbl) {
    tbl %>%
        summarise(avg_price    = exp(mean(price)) -1,
                  median_price = exp(median(price)) - 1,
                  n            = n()) %>%
        arrange(desc(n))
}

train %>%
    ggplot(aes(price)) +
    geom_histogram() 

# Plot column chart of median prices by neighbourhood group
train %>%
    group_by(neighbourhood_group) %>%
    summarise_prices() %>%
    mutate(neighbourhood_group = fct_reorder(neighbourhood_group, median_price)) %>%
    ggplot(aes(median_price, neighbourhood_group)) +
    geom_col()

# Plot distribution of price by neightbourhood group
train %>%
    mutate(neighbourhood_group = fct_reorder(neighbourhood_group, price)) %>%
    ggplot(aes(exp(price), neighbourhood_group)) +
    geom_boxplot() +
    scale_x_log10()

# Plot distribution of price by neighourhood
train %>%
    mutate(
        neighbourhood = fct_lump(neighbourhood, 40),
        neighbourhood = fct_reorder(neighbourhood, price)) %>%
    ggplot(aes(exp(price), neighbourhood)) +
    geom_boxplot() +
    scale_x_log10()
```

```{r}
# Plot distribution of price by neighourhood
train %>%
    mutate(room_type = fct_reorder(room_type, price)) %>%
    ggplot(aes(exp(price), room_type)) +
    geom_boxplot() +
    scale_x_log10

# Lump the occurrences above a certain number of nights using pmin()
train %>%
    mutate(minimum_nights = pmin(minimum_nights, 14)) %>%
    ggplot(aes(minimum_nights, price, group = minimum_nights)) +
    geom_boxplot()

train %>%
    sample_n(3000) %>%
    ggplot(aes(minimum_nights, price)) +
    geom_point() +
    scale_x_log10() +
    geom_smooth(method = "loess")

train %>%
    ggplot(aes(reviews_per_month, price)) +
    geom_point() +
    scale_x_log10() +
    geom_smooth(method = "lm")

train %>%
    ggplot(aes(calculated_host_listings_count, price)) +
    geom_point() +
    scale_x_log10() +
    geom_smooth(method = "lm")

train %>%
    ggplot(aes(availability_365, price)) +
    geom_point() +
    scale_x_log10() +
    geom_smooth(method = "lm")

train

```

# Build a map
```{r}
library(ggthemes)
train %>%
    ggplot(aes(longitude, latitude, color = price)) +
    geom_point(size = 1) +
    scale_color_gradient2(low = "dodgerblue", high = "red", midpoint = 5)

# esquisser()

train %>%
    group_by(latitude = round(latitude, 2),
             longitude = round(longitude, 2))  %>%
    summarise(price = mean(price)) %>%
    ggplot(aes(longitude, latitude, color = exp(price) - 1)) +
    geom_point() +
    scale_color_gradient2(low = "blue", high = "red", midpoint = 2, trans = "log10") + 
    theme_map()
```

```{r}
library(ggmap)
citation("ggmap")
# Determine bbox dimensions
# Get bottom and top coordinates
train %>% with(range(latitude))

# Get left and right coordinates
train %>% with(range(longitude))

bbox <-  c(left = -74.23986, bottom = 40.50641, right = -73.71829, top = 40.90804)

nyc_map <- get_stamenmap(bbox, zoom = 11)

aggregated_lat_lon <- train %>%
    group_by(latitude = round(latitude, 2),
             longitude = round(longitude, 2))  %>%
    summarise(price = mean(price),
              n = n()) %>%
    filter(n >= 5)

ggmap(nyc_map) +
    geom_point(aes(longitude, latitude, color = exp(price) - 1, size = n),
           data = aggregated_lat_lon) +
    scale_color_gradient2(low = "blue", high = "red", midpoint = 2, 
                          trans = "log10", labels = dollar) + 
    scale_size_continuous(range = c(1, 5)) +
    theme_map() +
    labs(
      title = "New York Airbnb Listings by Number and Mean Price",
      subtitle = "ggmap: Spatial Visualization with ggplot2",
      color = "Price",
      size = "# of listings",
      caption = "Data Source: https://www.kaggle.com/c/sliced-s01e05-WXx7h8/overview"
      
    )


```

# 2018 Airbnb Sydney Map
```{r}
sydney_data <- read_csv("Sydney/listings_summary_dec18.csv") %>%
  mutate(price = log1p(price)) %>%
  filter(price > 0)
  

sydney_data %>%
  ggplot(aes(price)) +
  geom_histogram()


library(ggmap)
citation("ggmap")
# Determine bbox dimensions
# Get bottom and top coordinates
sydney_data %>% with(range(latitude))

# Get left and right coordinates
sydney_data %>% with(range(longitude))

sydney_data %>% count(neighbourhood, sort = TRUE)

# Palm Beach 33.6011° S, 151.3217° E

bbox_au <-  c(left = 150.6429, bottom = -34.0, right = 151.39, top = -33.58)

syd_map <- get_stamenmap(bbox_au, zoom = 11)

aggregated_lat_lon_syd <- sydney_data %>%
    group_by(latitude = round(latitude, 2),
             longitude = round(longitude, 2))  %>%
    summarise(price = mean(price),
              n = n()) %>%
    filter(n >= 5)

gg_plain <- ggmap(syd_map) + 
  labs(x = "", y = "") +
  theme_void()

gg_syd <- ggmap(syd_map) +
  theme_void() +
    geom_point(aes(longitude, latitude, color = exp(price) - 1),
           data = aggregated_lat_lon_syd, shape = "square") +
    scale_color_gradient2(low = "blue", high = "red", midpoint = 1.9, 
                          trans = "log10", labels = dollar) + 
    scale_size_continuous(range = c(1, 5)) +
    theme_map() +
    labs(
      title = "Sydney Airbnb Mean Daily Rate By Location",
      subtitle = "Using ggmap: Spatial Visualization with ggplot2",
      color = "Daily Price",
      # size = "# of listings",
      caption = "Data Source: https://www.kaggle.com/tylerx/sydney-airbnb-open-data"
    )
syd_gg
plain_gg

```
# Box Plot By Neighbourhood - Sydney
```{r}
# 2.1 Time Series ----

gg_syd_boxplot <- sydney_data %>%
   mutate(neighbourhood = fct_lump_n(neighbourhood, 20)) %>%
  mutate(neighbourhood = fct_reorder(neighbourhood, price)) %>%
  ggplot(aes(exp(price), neighbourhood)) +
  geom_boxplot() +
  scale_x_log10(labels = scales::dollar_format()) +
  labs(title = "Mean Sydney Airbnb Price by Suburb",
       subtitle = "Top 20 Most Popular Sydney Suburbs For Airbnb By Number of Listings",
       x = "Price $A",
       y = "Suburb")
  

gg_syd_boxplot
  
```

# Patchwork Plot
```{r}

gg_syd_boxplot + (gg_syd / gg_plain) +
    plot_layout(widths = c(3,2), tag_level = "new") +
    plot_annotation(
        title      = "Sydney Australia Airbnb Prices in 2018",
        subtitle   = "Where to find the best value rooms\n"
        
    ) &
    theme(plot.tag.position = c(0, 1),
          plot.tag = element_text(size = 8, hjust = 0, vjust = 0))

gg_syd_boxplot / gg_syd  +
    # plot_layout(ncol = 1) +
    plot_annotation(
        title      = "Sydney Australia Airbnb Prices in 2018",
        subtitle   = "Where to find the best value rooms\n"
        
    ) &
    theme(plot.tag.position = c(0, 1),
          plot.tag = element_text(size = 8, hjust = 0, vjust = 0))
```


```{r}
train %>%
    ggplot(aes(last_review, price)) +
    geom_point() +
    geom_smooth(method = "lm")
```

# Text analysis
```{r}

library(tidytext)

train %>%
  unnest_tokens(word, name) %>%
  group_by(word) %>%
  summarise_prices() %>%
  head(100) %>%
  mutate(word = fct_reorder(word, avg_price)) %>%
  ggplot(aes(avg_price, word, size = n)) +
  geom_point()
```

```{r}
train %>%
  mutate(host_id = factor(host_id) ) %>%
  mutate(host_id = fct_lump_n(host_id, 40)) %>%
  mutate(host_id = fct_reorder(host_id, price)) %>%
  ggplot(aes(price, host_id)) +
  geom_boxplot()
```

TODO 

# Build model
```{r}

mset <- metric_set(rmse)

grid_control <- control_grid(
    save_pred = TRUE,
    save_workflow = TRUE,
    extract = extract_model
)

set.seed(42)
train_fold5 <-  train %>%
    vfold_cv(5)

```

# Base Model
```{r}

prep_juice <- function(d) juice(pred(d))

recipe_xgboost <- recipe(price ~ 
                             minimum_nights + room_type + number_of_reviews,
                         data = train) %>%
    # step(all_numeric_predictors(), offset = 1) %>%
    step_dummy(all_nominal_predictors(), one_hot = TRUE)

model_spec_xgboost <-  boost_tree(
    mode = "regression",
    mtry = tune(),
    trees = tune(),
    learn_rate = 0.01
) %>%
    set_engine("xgboost")

wflw_xgboost <- workflow() %>%
    add_recipe(recipe_xgboost) %>%
    add_model(model_spec_xgboost)

tune_wflw_xgboost <- wflw_xgboost %>%
    tune_grid(
        train_fold5,
        metrics = mset,
        grid = crossing(
            mtry = c(2, 3),
            trees = seq(50, 1000, 50)
        )
        )

autoplot(tune_wflw_xgboost)

tune_wflw_xgboost %>%
    collect_metrics() %>%
    arrange(mean)

select_best(tune_wflw_xgboost)
```

rmse = 0.53 with minimal model

# Adding variables to model
```{r}

prep_juice <- function(d) juice(pred(d))

recipe_xgboost_2 <- recipe(price ~ 
                             minimum_nights + room_type + number_of_reviews
                         + latitude + longitude,
                         data = train) %>%
    # step(all_numeric_predictors(), offset = 1) %>%
    step_dummy(all_nominal_predictors(), one_hot = TRUE)

model_spec_xgboost <-  boost_tree(
    mode = "regression",
    mtry = tune(),
    trees = tune(),
    learn_rate = 0.01
) %>%
    set_engine("xgboost")

wflw_xgboost_2 <- workflow() %>%
    add_recipe(recipe_xgboost_2) %>%
    add_model(model_spec_xgboost)

tune_wflw_xgboost_2 <- wflw_xgboost_2 %>%
    tune_grid(
        train_fold5,
        metrics = mset,
        grid = crossing(
            mtry = c(2, 4, 6),
            trees = seq(50, 1000, 50)
        )
        )

autoplot(tune_wflw_xgboost_2)

tune_wflw_xgboost_2 %>%
    collect_metrics() %>%
    arrange(mean)

select_best(tune_wflw_xgboost_2)
```


# Model iteration 3
```{r}
prep_juice <- function(d) juice(pred(d))

train %>% glimpse()

# Does not include name, host_id, host_name, neighbourhood, last review
recipe_xgboost_3 <- recipe(price ~ 
                             minimum_nights + room_type + number_of_reviews +
                             latitude + longitude + neighbourhood_group +
                             reviews_per_month + calculated_host_listings_count +
                             availability_365 ,
                           data = train) %>%
    # step(all_numeric_predictors(), offset = 1) %>%
    step_dummy(all_nominal_predictors(), one_hot = TRUE)

model_spec_xgboost <-  boost_tree(
    mode = "regression",
    mtry = tune(),
    trees = tune(),
    learn_rate = 0.01
) %>%
    set_engine("xgboost")

wflw_xgboost_3 <- workflow() %>%
    add_recipe(recipe_xgboost_3) %>%
    add_model(model_spec_xgboost)

tune_wflw_xgboost_3 <- wflw_xgboost_3 %>%
    tune_grid(
        train_fold5,
        metrics = mset,
        grid = crossing(
            mtry = c(2, 4, 6),
            trees = seq(50, 800, 50)
        )
        )

autoplot(tune_wflw_xgboost_3)

tune_wflw_xgboost_3 %>%
    collect_metrics() %>%
    arrange(mean)

select_best(tune_wflw_xgboost_3)
```

RMSE 
1st: 0.53
2nd: 0.463 - add lat/long
3rd: 0.438 - adding numeric variables like reviews per month

```{r}
holdout %>%
    semi_join(train, by = "host_id")

# Low number of host_id's in training set appear in hold out set
```

# Model 4

```{r}
prep_juice <- function(d) juice(prep(d))

train %>% glimpse()

# Does not include name, host_id, host_name, neighbourhood
recipe_xgboost_4 <- recipe(price ~ 
                             minimum_nights + room_type + number_of_reviews +
                             latitude + longitude + neighbourhood_group +
                             reviews_per_month + calculated_host_listings_count +
                             availability_365 + last_review,
                           data = train) %>%
    # step(all_numeric_predictors(), offset = 1) %>%
    step_mutate(last_review = coalesce(as.integer(Sys.Date() - last_review), 0)) %>%
    # step_unknown() %>%
    step_dummy(all_nominal_predictors(), one_hot = TRUE)

model_spec_xgboost <-  boost_tree(
    mode = "regression",
    mtry = tune(),
    trees = tune(),
    learn_rate = 0.01
) %>%
    set_engine("xgboost")

wflw_xgboost_4 <- workflow() %>%
    add_recipe(recipe_xgboost_4) %>%
    add_model(model_spec_xgboost)

tune_wflw_xgboost_4 <- wflw_xgboost_4 %>%
    tune_grid(
        train_fold5,
        metrics = mset,
        control = grid_control,
        grid = crossing(
            mtry = c(3, 5, 7),
            trees = seq(250, 800, 50)
        )
        )

autoplot(tune_wflw_xgboost_4)

tune_wflw_xgboost_4 %>%
    collect_metrics() %>%
    arrange(mean)

select_best(tune_wflw_xgboost_4)
```


```{r}

recipe_xgboost_4 %>%
    prep_juice() %>%
    ggplot(aes(last_review)) +
    geom_histogram()
```
RMSE 
1st: 0.53
2nd: 0.463 - add lat/long
3rd: 0.438 - adding numeric variables like reviews per month
4th: 0.438 - adding last review

# Finalise version without categorical:
```{r}
fit_wflw_xgboost <-  wflw_xgboost_4 %>%
    finalize_workflow(select_best(tune_wflw_xgboost_4)) %>%
    fit(train)

fit_wflw_xgboost %>%
    augment(test) %>%
    rmse(price, .pred)

importances <- xgboost::xgb.importance(model = fit_wflw_xgboost$fit$fit$fit)
    
importances %>%
    mutate(Feature = fct_reorder(Feature, Gain)) %>%
    ggplot(aes(Gain, Feature)) +
    geom_col()

fit_wflw_xgboost %>%
    predict_holdout() %>%
    write_csv("results/holdout_predictions.csv")

```
RMSE - 0.442

# Adding feature for Manhattan to remove columns
```{r}
prep_juice <- function(d) juice(prep(d))

train %>% glimpse()

# Does not include name, host_id, host_name, neighbourhood
recipe_xgboost_5 <- recipe(price ~ 
                             minimum_nights + room_type + number_of_reviews +
                             latitude + longitude + neighbourhood_group +
                             reviews_per_month + calculated_host_listings_count +
                             availability_365 + last_review,
                           data = train) %>%
    # step(all_numeric_predictors(), offset = 1) %>%
    step_mutate(is_manhattan = neighbourhood_group == "Manhattan") %>%
    step_rm(neighbourhood_group) %>%
    step_mutate(last_review = coalesce(as.integer(Sys.Date() - last_review), 0)) %>%
    step_dummy(all_nominal_predictors(), one_hot = TRUE)

model_spec_xgboost <-  boost_tree(
    mode = "regression",
    mtry = tune(),
    trees = tune(),
    learn_rate = tune()
) %>%
    set_engine("xgboost")

wflw_xgboost_5 <- workflow() %>%
    add_recipe(recipe_xgboost_5) %>%
    add_model(model_spec_xgboost)

tune_wflw_xgboost_5 <- wflw_xgboost_5 %>%
    tune_grid(
        train_fold5,
        metrics = mset,
        control = grid_control,
        grid = crossing(
            mtry = c(7),
            trees = seq(250, 1200, 50),
            learn_rate = c(.008, .01)
        )
        )

autoplot(tune_wflw_xgboost_5)

tune_wflw_xgboost_5 %>%
    collect_metrics() %>%
    arrange(mean)

```
RMSE = 0.436


```{r}
recipe_xgboost_5 %>%
    prep_juice() 

```

# Linear Model
```{r}

recipe_linear <- recipe(price ~ name + room_type +
                          longitude + latitude + neighbourhood_group +
                          neighbourhood + host_id,
                        data = train) %>%
  step_tokenize(name) %>%
  step_tokenfilter(name, max_tokens = tune()) %>%
  step_tf(name) %>%
  step_mutate(host_id = factor(host_id)) %>%
  step_other(host_id, neighbourhood, threshold = tune()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors())

model_spec_linear <- linear_reg(penalty = tune()) %>%
  set_engine("glmnet")

recipe_linear %>% prep() %>% juice()

wflw_linear <- workflow() %>%
  add_recipe(recipe_linear) %>%
  add_model(model_spec_linear)
  
tune_wflw_linear <- wflw_linear %>%
  tune_grid(train_fold5,
            metrics = mset,
            control = grid_control,
            grid = crossing(penalty = 10 ^ seq(-7, -1, .1),
                            threshold = .001,
                            max_tokens = c(30, 100, 300)))

autoplot(tune_wflw_linear)

tune_wflw_linear %>%
  collect_metrics() %>%
  arrange(mean)
  
```

RMSE 0.48 based on room type, lon/lat, 

```{r}
fit_wflw_linear <- wflw_linear %>%
  finalize_workflow(select_best(tune_wflw_linear)) %>%
  fit(train)

fit_wflw_linear %>%
  augment(test) %>%
  rmse(.pred, price)

```

```{r}
library(broom)

fit_wflw_linear$fit$fit$fit %>%
  tidy() %>%
  filter(lambda >= select_best(tune_wflw_linear)$penalty) %>%
  filter(lambda == min(lambda),
         term != "(Intercept)") %>%
  top_n(50, abs(estimate)) %>%
  mutate(term = fct_reorder(term, estimate)) %>%
  ggplot(aes(estimate, term, fill = estimate > 0)) +
  geom_col() +
  theme(legend.position = "none") + 
  labs(x = "Coefficient in regularised linear model")
```


```{r}
linear_best <- tune_wflw_linear %>% 
  filter_parameters(parameters = select_best(tune_wflw_linear))

xg_best <- tune_wflw_xgboost_5 %>%
  filter_parameters(parameters = select_best(tune_wflw_xgboost_5))

blended_linear_xgboost <- stacks() %>%
  add_candidates(linear_best) %>%
  add_candidates(xg_best) %>%
  blend_predictions()

blended_lin_xg_fit <- blended_linear_xgboost %>%
  fit_members()

class(blended_lin_xg_fit)

blended_lin_xg_fit %>%
  augment(test) %>%
  rmse(.pred, price)
```

```{r}
# Refit stack ensemble on full dataset
blended_linear_xgboost_fulldata <- blended_linear_xgboost
blended_linear_xgboost_fulldata$train <- dataset

blended_lin_xg_fulldata_fit <- blended_linear_xgboost_fulldata %>% 
  fit_members()

blended_lin_xg_fulldata_fit %>%
  predict_holdout() %>%
  write_csv("results/holdoutpredictions_2.csv")

```




